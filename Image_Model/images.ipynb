{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VN-xDuqKMNpP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataset{Food Recognition 2022,\n",
    "# \tauthor={AIcrowd},\n",
    "# \ttitle={Food Recognition 2022},\n",
    "# \tyear={2022},\n",
    "# \turl={https://www.kaggle.com/datasets/awsaf49/food-recognition-2022-dataset}\n",
    "# }\n",
    "\n",
    "\n",
    "##This is citation of our datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vmoB22DMOBee",
    "outputId": "79828060-e21b-4d6d-c9ca-4d27260599d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta Data Saved!\n",
      "Data saved:\n",
      "Number of Training Samples:  15561\n",
      "Number of Validation Samples:  468\n",
      "Number of Testing Samples:  0\n"
     ]
    }
   ],
   "source": [
    "#Paths for csv/sorted data\n",
    "tar_path = \"./food-recognition-DatasetNinja.tar\"\n",
    "training_csv = \"train/training_annotations.csv\"\n",
    "validation_csv = \"val/validation_annotations.csv\"\n",
    "testing_csv = \"test/testing_annotations.csv\"\n",
    "extracted_dir = \"./extracted_ann\"\n",
    "output_meta_json = \"meta.json\"\n",
    "training_image_dir = \"train/images\"\n",
    "validation_image_dir = \"val/images\"\n",
    "testing_image_dir = \"test/images\"\n",
    "\n",
    "\n",
    "training_data = []\n",
    "validation_data = []\n",
    "testing_data = []\n",
    "\n",
    "os.makedirs(extracted_dir, exist_ok=True)\n",
    "os.makedirs(training_image_dir, exist_ok=True)\n",
    "os.makedirs(validation_image_dir, exist_ok=True)\n",
    "os.makedirs(testing_image_dir, exist_ok=True)\n",
    "\n",
    "with tarfile.open(tar_path, \"r\") as tar:\n",
    "    tar_contents = tar.getnames()\n",
    "    for name in tar_contents:\n",
    "        if name.endswith(\".json\"):\n",
    "            tar.extract(name, path=extracted_dir)\n",
    "            try:\n",
    "                if \"meta\" in name:\n",
    "                    extracted_meta_path = os.path.join(extracted_dir, name)\n",
    "                    os.rename(extracted_meta_path, output_meta_json)\n",
    "                    print(\"Meta Data Saved!\")\n",
    "                    continue\n",
    "                if \"test\" in name:\n",
    "                    current_list = testing_data\n",
    "                elif name.startswith(\"training/\"):\n",
    "                    current_list = training_data\n",
    "                elif name.startswith(\"validation/\"):\n",
    "                    current_list = validation_data\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                extracted_path = os.path.join(extracted_dir, name)\n",
    "                with open(extracted_path, 'r') as f:\n",
    "                    annotation = json.load(f)\n",
    "\n",
    "                    image_name = os.path.basename(name).replace(\".json\", \"\")\n",
    "                    image_size = annotation.get(\"size\", {})\n",
    "                    image_height = image_size.get(\"height\", None)\n",
    "                    image_width = image_size.get(\"width\", None)\n",
    "\n",
    "                    objects = annotation.get(\"objects\", [])\n",
    "                    for obj in objects:\n",
    "                        class_title = obj.get(\"classTitle\", \"unknown\")\n",
    "                        points = obj.get(\"points\", {}).get(\"exterior\", [])\n",
    "                        points_str = \";\".join([f\"({x},{y})\" for x, y in points])\n",
    "\n",
    "                        current_list.append({\n",
    "                            \"filename\": image_name,\n",
    "                            \"labels\": class_title,\n",
    "                            \"polygon\": points_str,\n",
    "                            \"image_width\": image_width,\n",
    "                            \"image_height\": image_height\n",
    "                        })\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(\"Error message:\", e)\n",
    "                continue\n",
    "\n",
    "        elif name.endswith(\".jpg\"):\n",
    "            if \"test\" in name:\n",
    "                image_dir = testing_image_dir\n",
    "            elif name.startswith(\"training/\"):\n",
    "                image_dir = training_image_dir\n",
    "            elif name.startswith(\"validation/\"):\n",
    "                image_dir = validation_image_dir\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            tar.extract(name, path=extracted_dir)\n",
    "            extracted_image_path = os.path.join(extracted_dir, name)\n",
    "            destination_path = os.path.join(image_dir, os.path.basename(name))\n",
    "            shutil.move(extracted_image_path, destination_path)\n",
    "\n",
    "\n",
    "training_df = pd.DataFrame(training_data)\n",
    "validation_df = pd.DataFrame(validation_data)\n",
    "testing_df = pd.DataFrame(testing_data)\n",
    "\n",
    "training_df.to_csv(training_csv, index=False)\n",
    "validation_df.to_csv(validation_csv, index=False)\n",
    "testing_df.to_csv(testing_csv, index=False)\n",
    "\n",
    "print(\"Data saved:\")\n",
    "print(\"Number of Training Samples: \", len(training_df))\n",
    "print(\"Number of Validation Samples: \", len(validation_df))\n",
    "print(\"Number of Testing Samples: \", len(testing_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cQTvMVMZGMI",
    "outputId": "8ca2de79-a034-4c2c-dbe5-0fdf6bca58f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        chips-french-fries\n",
      "1                 hamburger\n",
      "2             hamburger-bun\n",
      "3             hamburger-bun\n",
      "4    salad-leaf-salad-green\n",
      "Name: labels, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Inspecting the labels for the data model\n",
    "annotations = pd.read_csv(\"train/training_annotations.csv\")\n",
    "print(annotations['labels'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adnAFtXOv9bj"
   },
   "outputs": [],
   "source": [
    "#Label Encoder Class for Encoding the specific food labels\n",
    "#Encoder makes string labels into numeric values\n",
    "class LabelEncoder:\n",
    "    def __init__(self, labels):\n",
    "        self.classes = sorted(labels)\n",
    "        self.label_to_index = {label: idx for idx, label in enumerate(self.classes)}\n",
    "        self.index_to_label = {idx: label for idx, label in enumerate(self.classes)}\n",
    "\n",
    "    def encode(self, label):\n",
    "        #Converts the labels into numbers\n",
    "        return self.label_to_index[label]\n",
    "\n",
    "    def decode(self, index):\n",
    "        #Converts the labels back into string variables\n",
    "        return self.index_to_label[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvDbuUb0BbT-"
   },
   "source": [
    "MODEL DEVELOPMENT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4o9euTEFeV4w"
   },
   "outputs": [],
   "source": [
    "#Dataset is based on documentation from resnet18\n",
    "class Image_Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        unique_labels = self.annotations['labels'].unique()\n",
    "        self.encoder = LabelEncoder(unique_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.annotations.iloc[idx, 1]\n",
    "        encoded_label = self.encoder.encode(label)\n",
    "        #given the transformation, apply to the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, encoded_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOUs0faBqePF"
   },
   "outputs": [],
   "source": [
    "#Defining transformations for both the training and validations set\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    #resnet18 needs images to be 256,256\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vP28yRoybWoQ",
    "outputId": "582f92b3-6d56-4192-914b-d70d62eb1e12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered metadata saved to train/train_annotations_filtered.csv.\n",
      "Number of valid entries: 15545\n"
     ]
    }
   ],
   "source": [
    "#We encounter the error that there are some mis labeled files so we\n",
    "#are removing them from our datasets\n",
    "\n",
    "#Start with the training dataset\n",
    "annotations = \"train/training_annotations.csv\"\n",
    "img_dir = \"train/images\"\n",
    "\n",
    "filtered_metadata = []\n",
    "with open(annotations, 'r') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    for row in reader:\n",
    "        file_path = os.path.join(img_dir, row[\"filename\"])\n",
    "        if os.path.exists(file_path):\n",
    "            filtered_metadata.append(row)\n",
    "\n",
    "\n",
    "filtered_metadata_file = \"train/train_annotations_filtered.csv\"\n",
    "with open(filtered_metadata_file, 'w', newline='') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=filtered_metadata[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(filtered_metadata)\n",
    "\n",
    "print(f\"Filtered metadata saved to {filtered_metadata_file}.\")\n",
    "print(f\"Number of valid entries: {len(filtered_metadata)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjSkiAwpwrKh",
    "outputId": "1632e0cd-a13b-4835-8441-c2c9b6d5256c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered metadata saved to val/val_annotations_filtered.csv.\n",
      "Number of valid entries: 468\n"
     ]
    }
   ],
   "source": [
    "#Now we are doing the same thing for the validation set\n",
    "annotations = \"val/validation_annotations.csv\"\n",
    "img_dir = \"val/images\"\n",
    "\n",
    "filtered_metadata = []\n",
    "with open(annotations, 'r') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    for row in reader:\n",
    "        file_path = os.path.join(img_dir, row[\"filename\"])\n",
    "        if os.path.exists(file_path):\n",
    "            filtered_metadata.append(row)\n",
    "\n",
    "\n",
    "# Save the filtered metadata\n",
    "filtered_metadata_file = \"val/val_annotations_filtered.csv\"\n",
    "with open(filtered_metadata_file, 'w', newline='') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=filtered_metadata[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(filtered_metadata)\n",
    "\n",
    "print(f\"Filtered metadata saved to {filtered_metadata_file}.\")\n",
    "print(f\"Number of valid entries: {len(filtered_metadata)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5md9SIhOcK-"
   },
   "outputs": [],
   "source": [
    "#We really don't need this function since it was created when we thought our dataset was multi-label\n",
    "def custom_collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    images = torch.stack(images)  \n",
    "    targets = torch.tensor(targets)  \n",
    "    return images, targets\n",
    "\n",
    "train_dataset = Image_Data(annotations_file=\"train/train_annotations_filtered.csv\",\n",
    "                                  img_dir=\"train/images\",\n",
    "                                  transform=train_transform)\n",
    "\n",
    "val_dataset = Image_Data(annotations_file=\"val/val_annotations_filtered.csv\",\n",
    "                                img_dir=\"val/images\",\n",
    "                                transform=val_transform)\n",
    "\n",
    "## Creating Dataloaders to put into our model\n",
    "train_loader = DataLoader( train_dataset,\n",
    "    batch_size=40,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate_fn)\n",
    "\n",
    "val_loader = DataLoader( val_dataset,\n",
    "    batch_size=40,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f9YkehmrqmFT",
    "outputId": "c9a46cc0-7e13-4965-d2be-cca34f2a0d9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 173MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "num_classes = 498\n",
    "\n",
    "num_classes_calculated = len(train_dataset.encoder.classes)  \n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes_calculated)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9LqDYEZK889"
   },
   "outputs": [],
   "source": [
    "# Some Training parameters\n",
    "batch_size = 20\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001  ##Started off with a very small learning rate\n",
    "criterion = nn.CrossEntropyLoss()  ##We googled best loss function for single labeled image recognition\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FGB5HiAm0jfU",
    "outputId": "74bd11f3-6540-49ae-cccf-f851a4bcc982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15545\n",
      "Missing files: 0\n"
     ]
    }
   ],
   "source": [
    "#DEBUGGING:\n",
    "missing_files = []\n",
    "print(len(train_dataset))\n",
    "for i in range(len(train_dataset)):\n",
    "    img_path = os.path.join(train_dataset.img_dir, train_dataset.annotations.iloc[i, 0])\n",
    "    if not os.path.exists(img_path):\n",
    "        missing_files.append(img_path)\n",
    "\n",
    "print(f\"Missing files: {len(missing_files)}\")\n",
    "if missing_files:\n",
    "    print(\"Titles:\", missing_files[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8Z4-HvSq2VQ",
    "outputId": "e495fb63-bcd6-4c93-bfe3-326c244fa13a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 5.5887\n",
      "Validation Loss: 7.3996\n",
      "Epoch [2/10], Train Loss: 4.4455\n",
      "Validation Loss: 7.9944\n",
      "Epoch [3/10], Train Loss: 3.9524\n",
      "Validation Loss: 8.5747\n",
      "Epoch [4/10], Train Loss: 3.6555\n",
      "Validation Loss: 8.8458\n",
      "Epoch [5/10], Train Loss: 3.4404\n",
      "Validation Loss: 9.2469\n",
      "Epoch [6/10], Train Loss: 3.2976\n",
      "Validation Loss: 9.4683\n",
      "Epoch [7/10], Train Loss: 3.1864\n",
      "Validation Loss: 9.8549\n",
      "Epoch [8/10], Train Loss: 3.0844\n",
      "Validation Loss: 9.8250\n",
      "Epoch [9/10], Train Loss: 2.9938\n",
      "Validation Loss: 10.0877\n",
      "Epoch [10/10], Train Loss: 2.9283\n",
      "Validation Loss: 10.3156\n"
     ]
    }
   ],
   "source": [
    "##TRAINING CELL, LOTS OF COMPUTE\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #Starting the Training Phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, targets in train_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    #Starting the evaluation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkiFikiFrDcF",
    "outputId": "12e7b35d-c52b-41a9-a75c-3fcfc3946aaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "##SAVING OUR MODEL\n",
    "\n",
    "torch.save(model.state_dict(), \"resnet18_images.pth\")\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QStBY_YDPKvP",
    "outputId": "7575a3f4-9793-4263-deb3-4285307a37b9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Testing out the model with image from our test dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet18_images.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest/images/041845.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#Testing out the model with image from our test dataset\n",
    "model.load_state_dict(torch.load(\"resnet18_images.pth\"))\n",
    "model.eval()\n",
    "\n",
    "image_path = \"test/images/041845.jpg\"\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "transform = val_transform\n",
    "input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor)\n",
    "    _, predicted_index = torch.max(outputs, 1)\n",
    "\n",
    "predicted_label = train_dataset.encoder.decode(predicted_index.item())\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
